{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d541e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import down\n",
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sys\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append('src')\n",
    "from score_util_pub import *\n",
    "from inference import *\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator('sdxl-light-1')\n",
    "\n",
    "amplification_factor = [1.0]*7\n",
    "\n",
    "# You can manually select amplification factor for each block \n",
    "\n",
    "amplification_factor[0] = 1.1 # amp factor for down 0 block\n",
    "amplification_factor[1] = 1.3 # amp factor for down 1 block\n",
    "amplification_factor[2] = 1.6 # amp factor for down 2 block\n",
    "amplification_factor[3] = 1.8 # amp factor for middle block\n",
    "\n",
    "# Or you can use the automatically found params\n",
    "# Please note that the automatically found params are searched for each block \n",
    "# and needs to be further scaled to be used in combination.\n",
    "\n",
    "# file_path = f'./results/sdxl-light-1/house/amp_factors_80.json'\n",
    "file_path = f'./results/amp_factors_80.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "amplification_factor[0] = 1+(data[0][0]-1)*0.2\n",
    "amplification_factor[1] = 1+(data[0][1]-1)*0.2\n",
    "amplification_factor[2] = 1+(data[0][2]-1)*0.1\n",
    "amplification_factor[3] = 1+(data[0][3]-1)*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a00737",
   "metadata": {},
   "source": [
    "# Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"house\"\n",
    "for seed in range(9):\n",
    "    prompt = f'a creative {obj}'\n",
    "    save_path = f'./results/{obj}/seed_{seed}/orig/{seed}.jpg'\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    orig = model.orig(prompt=prompt, seed=seed, save_path=save_path)\n",
    "    \n",
    "    save_path = f'./results/{obj}/seed_{seed}/c3/{seed}.jpg'\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    c3 = model.c3(prompt=prompt, seed=seed, replace_mask=amplification_factor, cutoff=[10.0,5.0,5.0,5.0,1.0,1.0,1.0], save_path=save_path)\n",
    "    \n",
    "    save_path = f'./results/{obj}/seed_{seed}/up/{seed}.jpg'\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    up_trans = model.dual_stage(prompt=prompt, seed=seed, replace_mask=amplification_factor, cutoff=[10.0,5.0,5.0,5.0,1.0,1.0,1.0], filter_factor=0.8, saliency_fft=False, save_path=save_path)\n",
    "    \n",
    "    save_path = f'./results/{obj}/seed_{seed}/down/{seed}.jpg'\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    down_saliency = model.dual_stage(prompt=prompt, seed=seed, replace_mask=amplification_factor, cutoff=[10.0,5.0,5.0,5.0,1.0,1.0,1.0], apply_filter=False, filter_factor=0.8, saliency_fft=True, save_path=save_path)\n",
    "    \n",
    "    save_path = f'./results/{obj}/seed_{seed}/ours/{seed}.jpg'\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    dual = model.dual_stage(prompt=prompt, seed=seed, replace_mask=amplification_factor,cutoff=[10.0,5.0,5.0,5.0,1.0,1.0,1.0], filter_factor=0.8, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29916ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_images = []\n",
    "c3_images = []\n",
    "up_images = []\n",
    "down_images = []\n",
    "ours_images = []\n",
    "\n",
    "# create image lists\n",
    "for seed in range(9):\n",
    "    # orig\n",
    "    image_dir = f'./results/{obj}/seed_{seed}/orig/{seed}.jpg'\n",
    "    img_pil = Image.open(image_dir)\n",
    "    orig_images.append(img_pil)\n",
    "    \n",
    "    # c3\n",
    "    image_dir = f'./results/{obj}/seed_{seed}/c3/{seed}.jpg'\n",
    "    img_pil = Image.open(image_dir)\n",
    "    c3_images.append(img_pil)\n",
    "    \n",
    "    # up\n",
    "    image_dir = f'./results/{obj}/seed_{seed}/up/{seed}.jpg'\n",
    "    img_pil = Image.open(image_dir)\n",
    "    up_images.append(img_pil)\n",
    "    \n",
    "    # down\n",
    "    image_dir = f'./results/{obj}/seed_{seed}/down/{seed}.jpg'\n",
    "    img_pil = Image.open(image_dir)\n",
    "    down_images.append(img_pil)\n",
    "    \n",
    "    #ours\n",
    "    image_dir = f'./results/{obj}/seed_{seed}/ours/{seed}.jpg'\n",
    "    img_pil = Image.open(image_dir)\n",
    "    ours_images.append(img_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fdf80",
   "metadata": {},
   "source": [
    "# Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8bd7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== CLIP Scores Orig ======\n",
      "mean: 0.2817\n",
      "std: 0.0165\n",
      "\n",
      "====== CLIP Scores c3 ======\n",
      "mean: 0.2815\n",
      "std: 0.0105\n",
      "\n",
      "====== CLIP Scores up ======\n",
      "mean: 0.2865\n",
      "std: 0.0088\n",
      "\n",
      "====== CLIP Scores down ======\n",
      "mean: 0.2823\n",
      "std: 0.0102\n",
      "\n",
      "====== CLIP Scores ours ======\n",
      "mean: 0.2872\n",
      "std: 0.0103\n"
     ]
    }
   ],
   "source": [
    "# CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# 输入文本\n",
    "text_prompt = f'a creative {obj}'\n",
    "text_tokens = clip.tokenize(text_prompt).to(device)\n",
    "with torch.no_grad():\n",
    "    text_feat = model.encode_text(text_tokens)\n",
    "    text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "\n",
    "for seed in range(9):\n",
    "    \n",
    "    clip_orig = []\n",
    "    clip_c3 = []\n",
    "    clip_up = []\n",
    "    clip_down = []\n",
    "    clip_ours = []\n",
    "\n",
    "    # orig\n",
    "    for img in orig_images:\n",
    "        image = img.convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_feat = model.encode_image(image_input)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = torch.matmul(img_feat, text_feat.T).item()\n",
    "        clip_orig.append(score)\n",
    "    \n",
    "    # c3\n",
    "    for img in c3_images:\n",
    "        image = img.convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_feat = model.encode_image(image_input)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = torch.matmul(img_feat, text_feat.T).item()\n",
    "        clip_c3.append(score)\n",
    "        \n",
    "    # up\n",
    "    for img in up_images:\n",
    "        image = img.convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_feat = model.encode_image(image_input)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = torch.matmul(img_feat, text_feat.T).item()\n",
    "        clip_up.append(score)\n",
    "        \n",
    "    # down\n",
    "    for img in down_images:\n",
    "        image = img.convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_feat = model.encode_image(image_input)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = torch.matmul(img_feat, text_feat.T).item()\n",
    "        clip_down.append(score)\n",
    "    \n",
    "    # orig\n",
    "    for img in ours_images:\n",
    "        image = img.convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_feat = model.encode_image(image_input)\n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = torch.matmul(img_feat, text_feat.T).item()\n",
    "        clip_ours.append(score)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "print(\"\\n====== CLIP Scores Orig ======\")\n",
    "print(f'mean: {np.mean(clip_orig):.4f}')\n",
    "print(f'std: {np.std(clip_orig):.4f}')\n",
    "\n",
    "print(\"\\n====== CLIP Scores c3 ======\")\n",
    "print(f'mean: {np.mean(clip_c3):.4f}')\n",
    "print(f'std: {np.std(clip_c3):.4f}')\n",
    "\n",
    "print(\"\\n====== CLIP Scores up ======\")\n",
    "print(f'mean: {np.mean(clip_up):.4f}')\n",
    "print(f'std: {np.std(clip_up):.4f}')\n",
    "\n",
    "print(\"\\n====== CLIP Scores down ======\")\n",
    "print(f'mean: {np.mean(clip_down):.4f}')\n",
    "print(f'std: {np.std(clip_down):.4f}')\n",
    "\n",
    "print(\"\\n====== CLIP Scores ours ======\")\n",
    "print(f'mean: {np.mean(clip_ours):.4f}')\n",
    "print(f'std: {np.std(clip_ours):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f664ff",
   "metadata": {},
   "source": [
    "# Lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0858af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635d9ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/xiaoyanzang24/miniconda3/envs/C3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/grads/xiaoyanzang24/miniconda3/envs/C3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/grads/xiaoyanzang24/miniconda3/envs/C3/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/xiaoyanzang24/miniconda3/envs/C3/lib/python3.10/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Lpips Scores C3 ======\n",
      "mean: 0.5477\n",
      "std: 0.0439\n",
      "\n",
      "====== Lpips Scores Up ======\n",
      "mean: 0.5616\n",
      "std: 0.0428\n",
      "\n",
      "====== Lpips Scores Down ======\n",
      "mean: 0.5126\n",
      "std: 0.0377\n",
      "\n",
      "====== Lpips Scores Ours ======\n",
      "mean: 0.5265\n",
      "std: 0.0365\n"
     ]
    }
   ],
   "source": [
    "# Load LPIPS model (alex / vgg / squeeze)\n",
    "loss_fn = lpips.LPIPS(net='alex').cuda()  # alex \n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "dist_c3 = []\n",
    "\n",
    "dist_up = []\n",
    "\n",
    "dist_down = []\n",
    "\n",
    "dist_ours = []\n",
    "\n",
    "for i in range(9):\n",
    "    img1 = to_tensor(orig_images[i]).unsqueeze(0).cuda()\n",
    "    img2 = to_tensor(c3_images[i]).unsqueeze(0).cuda()\n",
    "    \n",
    "    # Normalize to [-1, 1] for LPIPS\n",
    "    img1 = (img1 - 0.5) * 2\n",
    "    img2 = (img2 - 0.5) * 2\n",
    "    # Compute LPIPS\n",
    "    dist = loss_fn(img1, img2)\n",
    "    dist_c3.append(dist.item())\n",
    "\n",
    "print(\"\\n====== Lpips Scores C3 ======\")   \n",
    "print(f'mean: {np.mean(dist_c3):.4f}')\n",
    "print(f'std: {np.std(dist_c3):.4f}')\n",
    "\n",
    "for i in range(9):\n",
    "    img1 = to_tensor(orig_images[i]).unsqueeze(0).cuda()\n",
    "    img2 = to_tensor(up_images[i]).unsqueeze(0).cuda()\n",
    "    \n",
    "    # Normalize to [-1, 1] for LPIPS\n",
    "    img1 = (img1 - 0.5) * 2\n",
    "    img2 = (img2 - 0.5) * 2\n",
    "    # Compute LPIPS\n",
    "    dist = loss_fn(img1, img2)\n",
    "    dist_up.append(dist.item())\n",
    "\n",
    "print(\"\\n====== Lpips Scores Up ======\")    \n",
    "print(f'mean: {np.mean(dist_up):.4f}')\n",
    "print(f'std: {np.std(dist_up):.4f}')\n",
    "\n",
    "# down\n",
    "for i in range(9):\n",
    "    img1 = to_tensor(orig_images[i]).unsqueeze(0).cuda()\n",
    "    img2 = to_tensor(down_images[i]).unsqueeze(0).cuda()\n",
    "    \n",
    "    # Normalize to [-1, 1] for LPIPS\n",
    "    img1 = (img1 - 0.5) * 2\n",
    "    img2 = (img2 - 0.5) * 2\n",
    "    # Compute LPIPS\n",
    "    dist = loss_fn(img1, img2)\n",
    "    dist_down.append(dist.item())\n",
    "\n",
    "print(\"\\n====== Lpips Scores Down ======\")      \n",
    "print(f'mean: {np.mean(dist_down):.4f}')\n",
    "print(f'std: {np.std(dist_down):.4f}')\n",
    "\n",
    "\n",
    "# ours\n",
    "for i in range(9):\n",
    "    img1 = to_tensor(orig_images[i]).unsqueeze(0).cuda()\n",
    "    img2 = to_tensor(ours_images[i]).unsqueeze(0).cuda()\n",
    "    \n",
    "    # Normalize to [-1, 1] for LPIPS\n",
    "    img1 = (img1 - 0.5) * 2\n",
    "    img2 = (img2 - 0.5) * 2\n",
    "    # Compute LPIPS\n",
    "    dist = loss_fn(img1, img2)\n",
    "    dist_ours.append(dist.item())\n",
    "\n",
    "print(\"\\n====== Lpips Scores Ours ======\")     \n",
    "print(f'mean: {np.mean(dist_ours):.4f}')\n",
    "print(f'std: {np.std(dist_ours):.4f}')\n",
    "    \n",
    "# the smaller lpips, the more similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d20815",
   "metadata": {},
   "source": [
    "# Aesthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291538c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from aesthetic_predictor_v2_5 import convert_v2_5_from_siglip\n",
    "\n",
    "# load model\n",
    "amodel, a_preprocessor = convert_v2_5_from_siglip(\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "amodel = amodel.to(torch.bfloat16).cuda()\n",
    "amodel.eval()\n",
    "\n",
    "\n",
    "def calc_aesthetic_score(img: Image.Image) -> float:\n",
    "    # preprocess：convert img to pixel_values\n",
    "    inputs = a_preprocessor(images=img.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values.to(torch.bfloat16).cuda()\n",
    "\n",
    "    # forward\n",
    "    with torch.no_grad(), torch.inference_mode():\n",
    "        score = amodel(pixel_values).logits.squeeze().float().cpu().item()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2376e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Orig Aesthetic Score Summary =====\n",
      "Mean: 4.7743\n",
      "Std:  0.2455\n",
      "\n",
      "===== C3 Aesthetic Score Summary =====\n",
      "Mean: 4.9653\n",
      "Std:  0.3107\n",
      "\n",
      "===== Up Aesthetic Score Summary =====\n",
      "Mean: 4.9201\n",
      "Std:  0.1510\n",
      "\n",
      "===== Down Aesthetic Score Summary =====\n",
      "Mean: 5.1042\n",
      "Std:  0.1667\n",
      "\n",
      "===== Ours Aesthetic Score Summary =====\n",
      "Mean: 4.9410\n",
      "Std:  0.1708\n"
     ]
    }
   ],
   "source": [
    "aes_scores_orig = []\n",
    "for i, img in enumerate(orig_images):   # image_list: [PIL.Image, PIL.Image, ...]\n",
    "    score = calc_aesthetic_score(img)\n",
    "    aes_scores_orig.append(score)\n",
    "\n",
    "\n",
    "# mean and std\n",
    "print(\"\\n===== Orig Aesthetic Score Summary =====\")\n",
    "print(f\"Mean: {np.mean(aes_scores_orig):.4f}\")\n",
    "print(f\"Std:  {np.std(aes_scores_orig):.4f}\")\n",
    "\n",
    "aes_scores_c3 = []\n",
    "for i, img in enumerate(c3_images):   # image_list: [PIL.Image, PIL.Image, ...]\n",
    "    score = calc_aesthetic_score(img)\n",
    "    aes_scores_orig.append(score)\n",
    "\n",
    "\n",
    "# mean and std\n",
    "print(\"\\n===== C3 Aesthetic Score Summary =====\")\n",
    "print(f\"Mean: {np.mean(aes_scores_orig):.4f}\")\n",
    "print(f\"Std:  {np.std(aes_scores_orig):.4f}\")\n",
    "\n",
    "aes_scores_up = []\n",
    "for i, img in enumerate(up_images):   # image_list: [PIL.Image, PIL.Image, ...]\n",
    "    score = calc_aesthetic_score(img)\n",
    "    aes_scores_up.append(score)\n",
    "\n",
    "\n",
    "# mean and std\n",
    "print(\"\\n===== Up Aesthetic Score Summary =====\")\n",
    "print(f\"Mean: {np.mean(aes_scores_up):.4f}\")\n",
    "print(f\"Std:  {np.std(aes_scores_up):.4f}\")\n",
    "\n",
    "aes_scores_down = []\n",
    "for i, img in enumerate(down_images):   # image_list: [PIL.Image, PIL.Image, ...]\n",
    "    score = calc_aesthetic_score(img)\n",
    "    aes_scores_down.append(score)\n",
    "\n",
    "\n",
    "# mean and std\n",
    "print(\"\\n===== Down Aesthetic Score Summary =====\")\n",
    "print(f\"Mean: {np.mean(aes_scores_down):.4f}\")\n",
    "print(f\"Std:  {np.std(aes_scores_down):.4f}\")\n",
    "\n",
    "aes_scores_ours = []\n",
    "for i, img in enumerate(ours_images):   # image_list: [PIL.Image, PIL.Image, ...]\n",
    "    score = calc_aesthetic_score(img)\n",
    "    aes_scores_ours.append(score)\n",
    "\n",
    "\n",
    "# mean and std\n",
    "print(\"\\n===== Ours Aesthetic Score Summary =====\")\n",
    "print(f\"Mean: {np.mean(aes_scores_ours):.4f}\")\n",
    "print(f\"Std:  {np.std(aes_scores_ours):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
