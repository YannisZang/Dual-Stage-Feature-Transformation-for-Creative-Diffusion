{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_VP5toNO1bH3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VP5toNO1bH3",
    "outputId": "ca07bb6c-5f64-4cbe-c6e8-25e0f4bf8aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-fidelity in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch-fidelity) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch-fidelity) (11.0.0)\n",
      "Requirement already satisfied: scipy in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch-fidelity) (1.14.1)\n",
      "Requirement already satisfied: torch in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch-fidelity) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch-fidelity) (0.24.1)\n",
      "Requirement already satisfied: tqdm in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch-fidelity) (4.67.0)\n",
      "Requirement already satisfied: filelock in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from torch->torch-fidelity) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from sympy>=1.13.3->torch->torch-fidelity) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch-fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "R5r4iZ336HR_",
   "metadata": {
    "id": "R5r4iZ336HR_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch_fidelity import calculate_metrics\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c4196",
   "metadata": {},
   "source": [
    "### Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "p76lbud76J3I",
   "metadata": {
    "id": "p76lbud76J3I"
   },
   "outputs": [],
   "source": [
    "def list_images(folder):\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\", \".webp\"}\n",
    "    return [p for p in Path(folder).rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "def load_pil(path):\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def compute_fid_precision_recall(real_dir, fake_dir, batch_size=32):\n",
    "    metrics = calculate_metrics(\n",
    "        input1=real_dir,\n",
    "        input2=fake_dir,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        batch_size=batch_size,\n",
    "        isc=False,\n",
    "        fid=True,\n",
    "        kid=False,\n",
    "        prc=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return {\n",
    "        \"FID\": float(metrics.get(\"frechet_inception_distance\", -1)),\n",
    "        \"Precision\": float(metrics.get(\"precision\", -1)),\n",
    "        \"Recall\": float(metrics.get(\"recall\", -1)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09wP8BOG7VNx",
   "metadata": {
    "id": "09wP8BOG7VNx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Computing metrics for chair, original ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: original, FID: 106.54, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for chair, c3 ===\n",
      "Noun: chair, Method: c3, FID: 216.94, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for chair, upblock_transform ===\n",
      "Noun: chair, Method: upblock_transform, FID: 218.52, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for chair, saliency_gating ===\n",
      "Noun: chair, Method: saliency_gating, FID: 201.90, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for chair, both ===\n",
      "Noun: chair, Method: both, FID: 204.35, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for car, original ===\n",
      "Noun: car, Method: original, FID: 99.95, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for car, c3 ===\n",
      "Noun: car, Method: c3, FID: 116.58, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for car, upblock_transform ===\n",
      "Noun: car, Method: upblock_transform, FID: 119.88, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for car, saliency_gating ===\n",
      "Noun: car, Method: saliency_gating, FID: 111.12, Precision: -1.0000, Recall: -1.0000\n",
      "=== Computing metrics for car, both ===\n",
      "Noun: car, Method: both, FID: 114.53, Precision: -1.0000, Recall: -1.0000\n"
     ]
    }
   ],
   "source": [
    "method_names = [\"original\", \"c3\", \"upblock_transform\", \"saliency_gating\", \"both\"]\n",
    "nouns = [\"chair\", \"car\"]\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def resize_images(folder, size=(299, 299)):\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")):\n",
    "            path = os.path.join(folder, filename)\n",
    "            img = Image.open(path).convert(\"RGB\").resize(size)\n",
    "            img.save(path)\n",
    "\n",
    "\n",
    "\n",
    "for noun in nouns:\n",
    "    real_path = os.path.join(\"./dataset\", noun, \"test\")\n",
    "    resize_images(real_path, size=(299, 299))\n",
    "    for method in method_names:\n",
    "        print(f\"=== Computing metrics for {noun}, {method} ===\")\n",
    "        fake_path = os.path.join(\"./dataset\", noun, method)\n",
    "        resize_images(fake_path, size=(299, 299))\n",
    "        metrics = compute_fid_precision_recall(real_path, fake_path)\n",
    "        print(f\"Noun: {noun}, Method: {method}, FID: {metrics['FID']:.2f}, Precision: {metrics['Precision']:.4f}, Recall: {metrics['Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20762c",
   "metadata": {},
   "source": [
    "### Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c795126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "## Diversity-LPIPS\n",
    "import lpips\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "def compute_lpips(fake_dir, max_pairs=2000, img_size=256):\n",
    "    paths = list_images(fake_dir)\n",
    "    n = len(paths)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Random sampling pairs\n",
    "    idx = torch.randint(0, n, (max_pairs * 2,))\n",
    "    pairs = [(idx[i].item(), idx[i+1].item()) for i in range(0, len(idx), 2)]\n",
    "\n",
    "    dists = []\n",
    "    for i, j in tqdm(pairs, desc=\"LPIPS\"):\n",
    "        if i == j:\n",
    "            continue\n",
    "        img1 = transform(load_pil(paths[i])).unsqueeze(0).to(device)\n",
    "        img2 = transform(load_pil(paths[j])).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            d = lpips_model(img1, img2)\n",
    "        dists.append(d.item())\n",
    "\n",
    "    return float(np.mean(dists))\n",
    "## Diversity: Vendi Score\n",
    "import clip\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_vendi(fake_dir, max_images=200):\n",
    "    paths = list_images(fake_dir)[:max_images]\n",
    "    feats = []\n",
    "\n",
    "    for p in tqdm(paths, desc=\"CLIP feats\"):\n",
    "        img = clip_preprocess(load_pil(p)).unsqueeze(0).to(device)\n",
    "        feat = clip_model.encode_image(img)\n",
    "        feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        feats.append(feat.cpu().numpy())\n",
    "\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    K = feats @ feats.T\n",
    "    K = (K + 1) / 2  # normalize to [0,1]\n",
    "\n",
    "    K = K.astype(np.float64)\n",
    "\n",
    "    eigvals = np.linalg.eigvalsh(K)\n",
    "    eigvals = np.maximum(eigvals, 0)\n",
    "    p = eigvals / eigvals.sum()\n",
    "\n",
    "    entropy = -np.sum(p * np.log(p + 1e-12))\n",
    "    return float(np.exp(entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cd3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:30<00:00, 66.28it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 92.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: original, LPIPS: 0.4495, Vendi: 1.5611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:30<00:00, 65.45it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 86.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: c3, LPIPS: 0.5240, Vendi: 1.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:32<00:00, 62.25it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 81.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: upblock_transform, LPIPS: 0.5380, Vendi: 1.9791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:31<00:00, 64.14it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 85.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: saliency_gating, LPIPS: 0.4994, Vendi: 1.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:32<00:00, 61.11it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 89.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: both, LPIPS: 0.5214, Vendi: 1.9528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:32<00:00, 60.71it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 87.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: original, LPIPS: 0.6164, Vendi: 2.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:32<00:00, 61.43it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 73.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: c3, LPIPS: 0.6333, Vendi: 2.4522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:32<00:00, 60.88it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 78.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: upblock_transform, LPIPS: 0.6373, Vendi: 2.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:32<00:00, 60.79it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 88.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: saliency_gating, LPIPS: 0.6318, Vendi: 2.3777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS: 100%|██████████| 2000/2000 [00:33<00:00, 60.52it/s]\n",
      "CLIP feats: 100%|██████████| 100/100 [00:01<00:00, 74.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: both, LPIPS: 0.6345, Vendi: 2.3733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for noun in nouns:\n",
    "    for method in method_names:\n",
    "        fake_path = os.path.join(\"./dataset\", noun, method)\n",
    "        lpips_score = compute_lpips(fake_path, max_pairs=2000, img_size=299)\n",
    "        vendi_score = compute_vendi(fake_path, max_images=100)\n",
    "        print(f\"Noun: {noun}, Method: {method}, LPIPS: {lpips_score:.4f}, Vendi: {vendi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd638c9e",
   "metadata": {},
   "source": [
    "### Usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf214f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usability: CLIP Score\n",
    "@torch.no_grad()\n",
    "def compute_clip_score(fake_dir, prompt, max_images=200):\n",
    "    paths = list_images(fake_dir)[:max_images]\n",
    "\n",
    "    text = clip.tokenize([prompt]).to(device)\n",
    "    text_feat = clip_model.encode_text(text)\n",
    "    text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = []\n",
    "    for p in tqdm(paths, desc=\"CLIP score\"):\n",
    "        img = clip_preprocess(load_pil(p)).unsqueeze(0).to(device)\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "        sims.append((img_feat @ text_feat.T).item())\n",
    "\n",
    "    return float(np.mean(sims))\n",
    "## Usability: BLIP VQA\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "blip_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "blip_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_blip_yes_ratio(fake_dir, obj_name, max_images=200):\n",
    "    paths = list_images(fake_dir)[:max_images]\n",
    "    question = f\"Is this image a {obj_name}?\"\n",
    "\n",
    "    yes = 0\n",
    "    for p in tqdm(paths, desc=\"BLIP VQA\"):\n",
    "        img = load_pil(p)\n",
    "        inputs = blip_processor(img, question, return_tensors=\"pt\").to(device)\n",
    "        out = blip_model.generate(**inputs)\n",
    "        ans = blip_processor.decode(out[0], skip_special_tokens=True).lower()\n",
    "        if \"yes\" in ans:\n",
    "            yes += 1\n",
    "\n",
    "    return yes / len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b567f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 75.14it/s]\n",
      "BLIP VQA:   0%|          | 0/100 [00:00<?, ?it/s]/home/tai_phan/anaconda3/envs/C3/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:07<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: original, CLIP Score: 0.2879, BLIP Yes Ratio: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 81.52it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:08<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: c3, CLIP Score: 0.2766, BLIP Yes Ratio: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 69.08it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:07<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: upblock_transform, CLIP Score: 0.2781, BLIP Yes Ratio: 0.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 84.38it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:06<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: saliency_gating, CLIP Score: 0.2772, BLIP Yes Ratio: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 84.75it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:06<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: chair, Method: both, CLIP Score: 0.2792, BLIP Yes Ratio: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 80.71it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:07<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: original, CLIP Score: 0.2718, BLIP Yes Ratio: 0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 80.07it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:06<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: c3, CLIP Score: 0.2842, BLIP Yes Ratio: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 89.14it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:07<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: upblock_transform, CLIP Score: 0.2870, BLIP Yes Ratio: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 82.57it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:06<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: saliency_gating, CLIP Score: 0.2838, BLIP Yes Ratio: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP score: 100%|██████████| 100/100 [00:01<00:00, 85.66it/s]\n",
      "BLIP VQA: 100%|██████████| 100/100 [00:08<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: car, Method: both, CLIP Score: 0.2871, BLIP Yes Ratio: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for noun in nouns:\n",
    "    for method in method_names:\n",
    "        fake_path = os.path.join(\"./dataset\", noun, method)\n",
    "        prompt = f\"a creative {noun}\"\n",
    "        clip_score = compute_clip_score(fake_path, prompt, max_images=100)\n",
    "        blip_yes_ratio = compute_blip_yes_ratio(fake_path, noun, max_images=100)\n",
    "        print(f\"Noun: {noun}, Method: {method}, CLIP Score: {clip_score:.4f}, BLIP Yes Ratio: {blip_yes_ratio:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "C3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
