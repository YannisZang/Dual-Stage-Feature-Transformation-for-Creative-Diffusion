{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc894b2-da46-4285-bb86-7316394f7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch_fidelity import calculate_metrics\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def list_images(folder):\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\", \".webp\"}\n",
    "    return [p for p in Path(folder).rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "def load_pil(path):\n",
    "    return Image.open(path).convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e2c34-76db-4235-a955-7c66f6306074",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Novelty\n",
    "def compute_fid_precision_recall(real_dir, fake_dir, batch_size=32):\n",
    "    metrics = calculate_metrics(\n",
    "        input1=real_dir,\n",
    "        input2=fake_dir,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        batch_size=batch_size,\n",
    "        isc=False,\n",
    "        fid=True,\n",
    "        kid=False,\n",
    "        prc=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return {\n",
    "        \"FID\": float(metrics[\"frechet_inception_distance\"]),\n",
    "        \"Precision\": float(metrics[\"precision\"]),\n",
    "        \"Recall\": float(metrics[\"recall\"]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3531792-dec6-4c39-b766-0e714e97f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diversity-LPIPS\n",
    "import lpips\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "def compute_lpips(fake_dir, max_pairs=2000, img_size=256):\n",
    "    paths = list_images(fake_dir)\n",
    "    n = len(paths)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Random sampling pairs\n",
    "    idx = torch.randint(0, n, (max_pairs * 2,))\n",
    "    pairs = [(idx[i].item(), idx[i+1].item()) for i in range(0, len(idx), 2)]\n",
    "\n",
    "    dists = []\n",
    "    for i, j in tqdm(pairs, desc=\"LPIPS\"):\n",
    "        if i == j:\n",
    "            continue\n",
    "        img1 = transform(load_pil(paths[i])).unsqueeze(0).to(device)\n",
    "        img2 = transform(load_pil(paths[j])).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            d = lpips_model(img1, img2)\n",
    "        dists.append(d.item())\n",
    "\n",
    "    return float(np.mean(dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef7c54-1fd2-44c0-9620-1dd52051718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diversity: Vendi Score\n",
    "import clip\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_vendi(fake_dir, max_images=200):\n",
    "    paths = list_images(fake_dir)[:max_images]\n",
    "    feats = []\n",
    "\n",
    "    for p in tqdm(paths, desc=\"CLIP feats\"):\n",
    "        img = clip_preprocess(load_pil(p)).unsqueeze(0).to(device)\n",
    "        feat = clip_model.encode_image(img)\n",
    "        feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        feats.append(feat.cpu().numpy())\n",
    "\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    K = feats @ feats.T\n",
    "    K = (K + 1) / 2  # normalize to [0,1]\n",
    "\n",
    "    eigvals = np.linalg.eigvalsh(K)\n",
    "    eigvals = np.maximum(eigvals, 0)\n",
    "    p = eigvals / eigvals.sum()\n",
    "\n",
    "    entropy = -np.sum(p * np.log(p + 1e-12))\n",
    "    return float(np.exp(entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b59385-effa-4188-a5d6-a9b022261864",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usability: CLIP Score\n",
    "@torch.no_grad()\n",
    "def compute_clip_score(fake_dir, prompt, max_images=200):\n",
    "    paths = list_images(fake_dir)[:max_images]\n",
    "\n",
    "    text = clip.tokenize([prompt]).to(device)\n",
    "    text_feat = clip_model.encode_text(text)\n",
    "    text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = []\n",
    "    for p in tqdm(paths, desc=\"CLIP score\"):\n",
    "        img = clip_preprocess(load_pil(p)).unsqueeze(0).to(device)\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "        sims.append((img_feat @ text_feat.T).item())\n",
    "\n",
    "    return float(np.mean(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc4a8f-10cf-4b2a-9513-c3770cfdcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usability: BLIP VQA\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "blip_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "blip_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_blip_yes_ratio(fake_dir, obj_name, max_images=200):\n",
    "    paths = list_images(fake_dir)[:max_images]\n",
    "    question = f\"Is this image a {obj_name}?\"\n",
    "\n",
    "    yes = 0\n",
    "    for p in tqdm(paths, desc=\"BLIP VQA\"):\n",
    "        img = load_pil(p)\n",
    "        inputs = blip_processor(img, question, return_tensors=\"pt\").to(device)\n",
    "        out = blip_model.generate(**inputs)\n",
    "        ans = blip_processor.decode(out[0], skip_special_tokens=True).lower()\n",
    "        if \"yes\" in ans:\n",
    "            yes += 1\n",
    "\n",
    "    return yes / len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5f3ee-55b3-41ff-aa97-5d4108565181",
   "metadata": {},
   "source": [
    "Modify the path in the method according to the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898326b2-9cbd-47b6-b1b5-5d7e7f657479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    real_dir=\"path\",\n",
    "    fake_dir=\"path\",\n",
    "    obj_name=\"chair\",\n",
    "):\n",
    "    creative_prompt = f\"a creative {obj_name}\"\n",
    "\n",
    "    print(\"=== Novelty ===\")\n",
    "    novelty = compute_fid_precision_recall(real_dir, fake_dir)\n",
    "    print(novelty)\n",
    "\n",
    "    print(\"=== Diversity ===\")\n",
    "    lp = compute_lpips(fake_dir)\n",
    "    vd = compute_vendi(fake_dir)\n",
    "    print({\"LPIPS\": lp, \"Vendi\": vd})\n",
    "\n",
    "    print(\"=== Usability ===\")\n",
    "    cs = compute_clip_score(fake_dir, creative_prompt)\n",
    "    bl = compute_blip_yes_ratio(fake_dir, obj_name)\n",
    "    print({\"CLIP\": cs, \"BLIP\": bl})\n",
    "\n",
    "    return {\n",
    "        **novelty,\n",
    "        \"LPIPS\": lp,\n",
    "        \"Vendi\": vd,\n",
    "        \"CLIP\": cs,\n",
    "        \"BLIP\": bl,\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
